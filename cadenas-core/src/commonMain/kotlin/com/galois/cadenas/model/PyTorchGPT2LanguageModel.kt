package com.galois.cadenas.model

import com.galois.cadenas.mbfte.ModelLoader
import com.galois.cadenas.mbfte.ModelLoaderFactory
import com.galois.cadenas.utils.Tuple3
import com.galois.cadenas.utils.update
import kotlinx.serialization.ExperimentalSerializationApi
import kotlinx.serialization.json.Json
import kotlinx.serialization.json.decodeFromStream
import org.pytorch.DType
import org.pytorch.IValue
import org.pytorch.Module
import org.pytorch.Tensor
import java.io.BufferedReader
import java.io.InputStreamReader
import kotlin.math.exp

class PyTorchGPT2LanguageModel(
    modelDir: String,
    private val temperature: Double = 1.0,
    private val topK: Int = 100000,
    private val topP: Double = 1.0,
    private val modelLoader: ModelLoader = ModelLoaderFactory.createModelLoader(),
) : LanguageModel {

    /** The tokenizer to use with our model */
    override val tokenizer: Tokenizer

    /** An instance of the PyTorch model */
    private val model: Module

    /** ID of the EOS token */
    private var eosId: Int? = null

    /** Size of the vocabulary for this model */
    private val vocabSize: Int

    init {
        // Load the model data
        val res = readData(modelDir)
        model = res._1
        tokenizer = res._2
        vocabSize = res._3
    }

    /**
     * Generate predictions for the next token given a seed text
     * and past data.
     *
     * @param text The seed text. When passing in a non-null [past] value, the
     *     seed text should not contain any text that went in to the creation of
     *     the [past] value.
     * @param past Past data as generated by a previous call to [prediction] or null.
     * @return The results of the prediction. See [Prediction].
     */
    override fun prediction(text: String, past: Any?): Prediction {
        // We need to pass a `past` value to the model, irrespective of
        // whether the caller gave us one or not
        val pastVal = past ?: makeDummyPastValue()

        // Sanity check: Check that the past is an IValue
        require(pastVal is IValue) { "Invalid past value - expected IValue" }

        // Tokenize the input text and convert to a Tensor IValue suitable
        // for use with the model.
        val inputs = tokenizer.tokenize(text)
        val inputIdsVal = IValue.from(makeInputTensor(inputs))

        // Query the model and fetch its output as a tuple
        val outputs = model.forward(inputIdsVal, pastVal)
        val outputsTuple = outputs.toTuple()

        // The first entry is the raw logits from the model and the second is
        // a `past` value
        val logitsTensor = outputsTuple[0].toTensor()
        val lastPast = outputsTuple[1]

        // Process the logits and return the results
        val predictions = getPredictions(logitsTensor)
        val prediction = computeCumulativeProbabilities(predictions).toMutableList()

        if (topK <= tokenizer.vocabSize()) {
            val total = prediction[topK - 1].probability

            for (i in 0..<topK) {
                prediction[i] =
                    TokenProbability(prediction[i].tokenId, prediction[i].probability / total)
            }

            for (i in topK..prediction.lastIndex) {
                prediction[i] = TokenProbability(prediction[i].tokenId, 1.0)
            }
        }

        if (topP < 1.0) {
            val splitIndex = findSplitIndex(prediction, topP)
            val total = prediction[splitIndex - 1].probability

            for (i in 0..<splitIndex) {
                prediction[i] =
                    TokenProbability(prediction[i].tokenId, prediction[i].probability / total)
            }

            for (i in splitIndex..prediction.lastIndex) {
                prediction[i] = TokenProbability(prediction[i].tokenId, 1.0)
            }
        }

        return Prediction(prediction, lastPast)
    }

    /**
     * Destroy the instance.
     *
     * PyTorch leaks memory, so this method provides a way to clear a loaded
     * model from memory explicitly.
     *
     * IMPORTANT: Any instance on which destroy() is called is no longer
     * valid and must be recreated!
     */
    override fun destroy() {
        modelLoader.unloadModel()
    }

    private fun readData(modelDir: String): Tuple3<Module, Tokenizer, Int> {
        val model = modelLoader.loadModel(modelDir)

        val encoder = loadVocab(modelDir)
        require(EOS in encoder)
        eosId = encoder[EOS]
        val decoder = encoder.entries.associateBy({ it.value }, { it.key })
        val merges = loadMerges(modelDir)
        val tokenizer = Tokenizer(encoder, decoder, merges)

        return Tuple3(model, tokenizer, encoder.size)
    }

    @OptIn(ExperimentalSerializationApi::class)
    private fun loadVocab(modelDir: String): Map<String, Int> {
        val vocabStream = modelLoader.getAuxiliaryFileStream(modelDir, VOCAB_PATH)
        vocabStream.use {
            return Json.decodeFromStream<Map<String, Int>>(it)
        }
    }

    private fun loadMerges(modelDir: String): Map<Pair<String, String>, Int> {
        val mergesStream = modelLoader.getAuxiliaryFileStream(modelDir, MERGES_PATH)
        mergesStream.use { inputStream ->
            return BufferedReader(InputStreamReader(inputStream)).useLines {
                it.drop(1).mapIndexed { index, s ->
                    val (s1, s2) = s.split(" ")
                    (s1 to s2) to index
                }.toMap()
            }
        }
    }

    private fun makeDummyPastValue(): IValue {
        fun makeZeroTensor(): Tensor {
            val shape = longArrayOf(1, 12, 0, 64)
            val elemCount = Tensor.numel(shape)
            val buf = Tensor.allocateFloatBuffer(elemCount.toInt())
            return Tensor.fromBlob(buf, shape)
        }

        val data = Array(12) {
            IValue.tupleFrom(
                IValue.from(makeZeroTensor()),
                IValue.from(makeZeroTensor())
            )
        }
        return IValue.tupleFrom(*data)
    }

    private fun makeInputTensor(inputIds: List<Int>): Tensor {
        // The model seems to expect a Tensor with type Long
        val shape = longArrayOf(1.toLong(), inputIds.size.toLong())
        return Tensor.fromBlob(
            inputIds.map { it.toLong() }.toLongArray(),
            shape
        )
    }

    private fun getPredictions(logitsTensor: Tensor): MutableList<TokenProbability> {
        // Note that we support both 32-bit (i.e. Float) Tensors and 64-bit (i.e Double) Tensors
        // In either case we will return data as doubles.

        // Structurally, the logits output is a 3D array with the dimensions: (batch_size,
        // sequence_length, config.vocab_size). batch_size is always 1 for us at the moment;
        // then for each element of the input (in order), we have a prediction score for
        // each token in the vocabulary. Note that for our current style of text
        // generation where we are appending new tokens to the input sequence, we
        // really only care about the prediction scores for the last element of the input
        // sequence. That is, we are interested in the array slice represented (python-style)
        // by [0, -1, :].

        // One wrinkle here is that with PyTorch java, the array we obtain is fully *flat*
        // and it is up to us to pull out the right indices "by hand".
        // TODO: Consider switching to a library like multik
        val logits = if (logitsTensor.dtype() == DType.FLOAT32) {
            // That means we can retrieve the data as a float array
            val logitsData = logitsTensor.dataAsFloatArray

            // And figure out the start of the logits we care about
            val startIndex = logitsData.size - vocabSize

            // We do a mutable copy of the relevant slice here instead of
            // nicer looking functional version here to avoid creating
            // intermediate collections unnecessarily
            val resultArray = DoubleArray(vocabSize)
            logitsData.asList().subList(startIndex, logitsData.size).forEachIndexed { i, v ->
                resultArray[i] = v.toDouble()
            }

            resultArray
        } else {
            // In this case we assume it is a double tensor and attempt to
            // retrieve the data as an array of doubles
            val logitsData = logitsTensor.dataAsDoubleArray

            // We directly extract the slice we are interested in
            val startIndex = logitsData.size - vocabSize
            logitsData.sliceArray(startIndex until logitsData.size)
        }

        // Set end-of-text token logit to a large negative number
        // Safety: We require() that the eosId be set when we load the model.
        logits[eosId!!] = LOW_SCORE

        // Convert logits to probabilities using softmax
        softmax(logits)

        // Construct the final result type and return
        val vals: MutableList<TokenProbability> = mutableListOf()
        for (i in 0..logits.lastIndex) {
            if (logits[i] == 0.0)
                continue
            vals.add(TokenProbability(i, logits[i]))
        }

        return vals
    }

    private fun softmax(values: DoubleArray) {
        val max = values.maxOrNull()!!

        values.update { exp((it - max) / temperature) }
        val sum = values.sum()

        values.update { it / sum }
    }

    /**
     * Given a list of predictions compute cumulative probabilities after ordering
     * them from highest to lowest probabilities.
     *
     * @param predictions The list of predications as a list of [TokenProbability]
     * @return A list of [TokenProbability] representing cumulative probability for each token
     */
    private fun computeCumulativeProbabilities(predictions: List<TokenProbability>): List<TokenProbability> {
        // Sort the predictions from the highest to lowest probability
        val sortedList = predictions.sortedByDescending { it.probability }
        val results = mutableListOf<TokenProbability>()

        for ((i, tokenProb) in sortedList.withIndex()) {
            val prob = if (i == 0) {
                tokenProb.probability
            } else if (i == sortedList.lastIndex) {
                1.0
            } else {
                results[i - 1].probability + tokenProb.probability
            }

            results.add(TokenProbability(tokenProb.tokenId, prob))
        }

        return results
    }

    private fun findSplitIndex(predictions: List<TokenProbability>, threshold: Double): Int {
        for ((i, tokenProb) in predictions.withIndex()) {
            if (tokenProb.probability >= threshold) {
                if (i == 0) {
                    return predictions.size
                }
                return i
            }
        }
        return predictions.size
    }

    companion object {
        const val VOCAB_PATH = "gpt2-vocab.json"
        const val MERGES_PATH = "gpt2-merges.txt"

        const val EOS: String = "<|endoftext|>"
        const val LOW_SCORE: Double = -10e10
    }
}
